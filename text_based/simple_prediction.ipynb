{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dromii4/dromii/owlv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "간단한 텍스트 기반 객체 탐지 예제\n",
    "==================================\n",
    "이 노트북은 OWLv2 모델을 사용한 기본적인 텍스트 기반 객체 탐지 예제입니다.\n",
    "텍스트 프롬프트를 사용하여 이미지에서 객체를 탐지하는 방법을 보여줍니다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "# ==================== 시스템 초기화 ====================\n",
    "# 0. 디바이스 설정 (GPU 우선, 없으면 CPU 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # CUDA 사용 가능하면 'cuda' 출력\n",
    "\n",
    "# 1. OWLv2 모델 & 프로세서 로드\n",
    "# Google의 Open-Vocabulary Object Detection 모델\n",
    "processor = Owlv2Processor.from_pretrained(\n",
    "    \"google/owlv2-large-patch14-ensemble\",\n",
    "    image_size=1008,    # OWLv2 최적 입력 크기 (14×72 패치)\n",
    ")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\n",
    "    \"google/owlv2-large-patch14-ensemble\"\n",
    ").to(device).eval()  # 평가 모드로 설정\n",
    "\n",
    "print(\"OWLv2 모델 로딩 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d4bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] DJI_0951.JPG → DJI_0951.JPG, DJI_0951.json\n",
      "[OK] DJI_0691.JPG → DJI_0691.JPG, DJI_0691.json\n",
      "[OK] DJI_0033.JPG → DJI_0033.JPG, DJI_0033.json\n",
      "[OK] DJI_0559.jpg → DJI_0559.jpg, DJI_0559.json\n",
      "[OK] DJI_0934.jpg → DJI_0934.jpg, DJI_0934.json\n",
      "[OK] DJI_0923.jpg → DJI_0923.jpg, DJI_0923.json\n",
      "[OK] DJI_0008.JPG → DJI_0008.JPG, DJI_0008.json\n",
      "[OK] DJI_0933.jpg → DJI_0933.jpg, DJI_0933.json\n",
      "[OK] DJI_0866.JPG → DJI_0866.JPG, DJI_0866.json\n",
      "[OK] DJI_0036.JPG → DJI_0036.JPG, DJI_0036.json\n",
      "[OK] DJI_0928.jpg → DJI_0928.jpg, DJI_0928.json\n"
     ]
    }
   ],
   "source": [
    "# ==================== 설정 및 경로 ====================\n",
    "import json\n",
    "from pathlib import Path\n",
    "from torchvision.ops import nms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# 디렉토리 설정\n",
    "IMAGE_DIR    = Path(\"encumbrance_labels/encumbrance\")        # 원본 이미지 폴더\n",
    "OUTPUT_DIR   = Path(\"encumbrance_labels/simple_prediction\")  # 결과 저장 폴더\n",
    "\n",
    "# ==================== 탐지할 객체 클래스 ====================\n",
    "# 6가지 주요 객체 클래스 정의\n",
    "# 건축물, 컨테이너, 비닐하우스, 논/밭, 수목, 분묘\n",
    "TEXT_QUERIES = [\n",
    "    \"Tomb\",        # 무덤/고분\n",
    "    \"Tree\",        # 나무\n",
    "    \"Greenhouse\",  # 온실/비닐하우스\n",
    "    \"Building\",    # 건물\n",
    "    \"Field\",       # 밭/논\n",
    "    \"Container\"    # 컨테이너\n",
    "]\n",
    "\n",
    "print(f\"탐지 대상 클래스: {TEXT_QUERIES}\")\n",
    "print(f\"입력 디렉토리: {IMAGE_DIR}\")\n",
    "print(f\"출력 디렉토리: {OUTPUT_DIR}\")\n",
    "\n",
    "# ==================== 시각화 및 하이퍼파라미터 ====================\n",
    "# 클래스별 시각화 색상 (RGB 값)\n",
    "COLOR_MAP = {\n",
    "    \"Tomb\":       (0,   0, 255),  # 파란색 - 무덤\n",
    "    \"Tree\":       (0, 255,   0),  # 초록색 - 나무\n",
    "    \"Greenhouse\": (255, 0,   0),  # 빨간색 - 온실\n",
    "    \"Building\":   (0, 255, 255),  # 청록색 - 건물\n",
    "    \"Field\":      (255, 0, 255),  # 자홍색 - 밭\n",
    "    \"Container\":  (255, 255,   0), # 노란색 - 컨테이너\n",
    "}\n",
    "\n",
    "# 탐지 임계값 설정\n",
    "THRESHOLD     = 0.2  # 신뢰도 임계값 (0.2 이상만 탐지)\n",
    "IOU_THRESHOLD = 0.3  # NMS IoU 임계값 (0.3 이상 겹치는 박스 제거)\n",
    "TARGET_SIZE   = 1008  # OWLv2 최적 입력 크기로 수정\n",
    "\n",
    "# ==================== 초기 설정 ====================\n",
    "# 출력 폴더 생성\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "font = ImageFont.load_default()  # 텍스트 표시용 폰트\n",
    "\n",
    "# ==================== 메인 처리 루프 ====================\n",
    "# 이미지 디렉토리의 모든 이미지 파일 처리\n",
    "for img_path in IMAGE_DIR.glob(\"*\"):\n",
    "    # 지원하는 이미지 형식만 처리 (.jpg, .jpeg, .png)\n",
    "    if img_path.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "        continue\n",
    "\n",
    "    print(f\"처리 중: {img_path.name}\")\n",
    "\n",
    "    # ==================== 1단계: 이미지 전처리 ====================\n",
    "    # 원본 이미지 로드 및 RGB 변환\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    # OWLv2 모델 입력 크기로 리사이징 (1008x1008)\n",
    "    image = image.resize((TARGET_SIZE, TARGET_SIZE), Image.BILINEAR)\n",
    "    w, h = image.size  # w == h == 1008\n",
    "\n",
    "    # ==================== 2단계: OWLv2 모델 입력 준비 ====================\n",
    "    # 텍스트 쿼리와 이미지를 프로세서에 전달하여 모델 입력 생성\n",
    "    inputs = processor(text=[TEXT_QUERIES], images=image, return_tensors=\"pt\")\n",
    "    # GPU로 데이터 이동 (가능한 경우)\n",
    "    for k, v in inputs.items():\n",
    "        if hasattr(v, \"to\"):\n",
    "            inputs[k] = v.to(device)\n",
    "\n",
    "    # ==================== 3단계: 모델 추론 및 후처리 ====================\n",
    "    # GPU 메모리 사용량 최적화를 위한 no_grad 컨텍스트\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # OWLv2 모델 추론\n",
    "        \n",
    "    # 모델 출력을 바운딩 박스, 점수, 라벨로 변환\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs,\n",
    "        target_sizes=torch.tensor([[h, w]], device=device),  # 원본 이미지 크기\n",
    "        threshold=THRESHOLD  # 신뢰도 임계값 적용\n",
    "    )[0]\n",
    "\n",
    "    # 결과 분리\n",
    "    boxes, scores, labels = (\n",
    "        results[\"boxes\"],    # 바운딩 박스 좌표 [x1, y1, x2, y2]\n",
    "        results[\"scores\"],   # 신뢰도 점수\n",
    "        results[\"labels\"],   # 클래스 라벨 인덱스\n",
    "    )\n",
    "    \n",
    "    # ==================== 4단계: Non-Maximum Suppression (NMS) ====================\n",
    "    # 겹치는 박스들 중에서 가장 높은 점수의 박스만 유지\n",
    "    keep = nms(boxes, scores, IOU_THRESHOLD)\n",
    "    boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "\n",
    "    # ==================== 5단계: 시각화 및 결과 저장 ====================\n",
    "    draw = ImageDraw.Draw(image)  # 이미지에 그리기 객체 생성\n",
    "    records = []  # JSON 저장용 결과 리스트\n",
    "\n",
    "    # 각 탐지된 객체에 대해 시각화 및 기록\n",
    "    for box, score, li in zip(boxes, scores, labels):\n",
    "        label = TEXT_QUERIES[li]  # 클래스 인덱스를 이름으로 변환\n",
    "        xmin, ymin, xmax, ymax = box.tolist()\n",
    "        \n",
    "        # ==================== 박스 좌표 클램핑 ====================\n",
    "        # 이미지 경계를 벗어나는 박스 좌표를 이미지 내부로 제한\n",
    "        xmin, ymin = max(0, xmin), max(0, ymin)\n",
    "        xmax, ymax = min(w, xmax), min(h, ymax)\n",
    "        \n",
    "        # 클래스별 색상 가져오기\n",
    "        color = COLOR_MAP.get(label, (255,255,255))\n",
    "\n",
    "        # ==================== 바운딩 박스 그리기 ====================\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=color, width=3)\n",
    "        \n",
    "        # ==================== 라벨 텍스트 표시 ====================\n",
    "        txt = f\"{label}: {score:.2f}\"  # 클래스명과 신뢰도 점수\n",
    "        # 텍스트 배경 박스 계산\n",
    "        x0, y0, x1, y1 = draw.textbbox((xmin, ymin), txt, font=font)\n",
    "        # 텍스트 배경 그리기\n",
    "        draw.rectangle([x0, y0, x1, y1], fill=color)\n",
    "        # 흰색 텍스트 그리기\n",
    "        draw.text((xmin, ymin), txt, fill=(255,255,255), font=font)\n",
    "\n",
    "        # ==================== JSON 기록용 데이터 추가 ====================\n",
    "        records.append({\n",
    "            \"label\": label,\n",
    "            \"score\": float(score),\n",
    "            \"box\": [round(xmin,2), round(ymin,2), round(xmax,2), round(ymax,2)]\n",
    "        })\n",
    "\n",
    "    # ==================== 6단계: 결과 파일 저장 ====================\n",
    "    out_img  = OUTPUT_DIR / img_path.name  # 시각화된 이미지 저장 경로\n",
    "    out_json = OUTPUT_DIR / f\"{img_path.stem}.json\"  # JSON 결과 저장 경로\n",
    "    \n",
    "    # 시각화된 이미지 저장\n",
    "    image.save(out_img)\n",
    "    # JSON 결과 저장 (UTF-8 인코딩, 한글 지원)\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(records, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[OK] {img_path.name} → {out_img.name}, {out_json.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
