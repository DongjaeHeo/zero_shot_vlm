{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dromii4/dromii/owlv2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Processing DJI_0951.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0951.JPG, DJI_0951.json\n",
      "\n",
      "▶ Processing DJI_0691.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0691.JPG, DJI_0691.json\n",
      "\n",
      "▶ Processing DJI_0033.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0033.JPG, DJI_0033.json\n",
      "\n",
      "▶ Processing DJI_0559.jpg …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0559.jpg, DJI_0559.json\n",
      "\n",
      "▶ Processing DJI_0934.jpg …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0934.jpg, DJI_0934.json\n",
      "\n",
      "▶ Processing DJI_0923.jpg …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0923.jpg, DJI_0923.json\n",
      "\n",
      "▶ Processing DJI_0008.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0008.JPG, DJI_0008.json\n",
      "\n",
      "▶ Processing DJI_0933.jpg …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0933.jpg, DJI_0933.json\n",
      "\n",
      "▶ Processing DJI_0866.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0866.JPG, DJI_0866.json\n",
      "\n",
      "▶ Processing DJI_0036.JPG …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0036.JPG, DJI_0036.json\n",
      "\n",
      "▶ Processing DJI_0928.jpg …\n",
      "  • Guided detection for class 'Building' with 5 exemplars\n",
      "  • Guided detection for class 'Container' with 3 exemplars\n",
      "  • Guided detection for class 'Field' with 5 exemplars\n",
      "  • Guided detection for class 'Greenhouse' with 5 exemplars\n",
      "  • Guided detection for class 'Tomb' with 1 exemplars\n",
      "  • Guided detection for class 'Tree' with 5 exemplars\n",
      "  [OK] Saved → DJI_0928.jpg, DJI_0928.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "이미지 기반 객체 탐지 예제\n",
    "===================================\n",
    "이 노트북은 이미지 기반 검색을 사용한 객체 탐지 예제입니다.\n",
    "각 클래스별로 대표 이미지를 사용하여 유사한 객체를 탐지합니다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ==================== 디렉토리 설정 ====================\n",
    "IMAGE_DIR        = Path(\"encumbrance_labels/encumbrance\")           # 원본 이미지 디렉토리\n",
    "EACH_CLASSES_DIR = Path(\"encumbrance_labels/each_classes\")          # 클래스별 대표 이미지 디렉토리\n",
    "OUTPUT_DIR       = Path(\"encumbrance_labels/prediction_with_multiple_images\")  # 결과 저장 디렉토리\n",
    "\n",
    "# ==================== 하이퍼파라미터 ====================\n",
    "TARGET_SIZE      = 1008   # OWLv2 최적 입력 크기 (14×72 패치)\n",
    "GUIDED_THRESHOLD = 0.9    # 이미지 기반 매칭 신뢰도 임계값\n",
    "IOU_THRESHOLD    = 0.1    # NMS IoU 임계값\n",
    "\n",
    "# ==================== 시스템 설정 ====================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 클래스 이름 자동 감지 (each_classes 디렉토리의 하위 폴더명)\n",
    "CLASS_NAMES = [p.name for p in EACH_CLASSES_DIR.iterdir() if p.is_dir()]\n",
    "CLASS_NAMES.sort()\n",
    "print(f\"탐지 대상 클래스: {CLASS_NAMES}\")\n",
    "\n",
    "# ==================== 시각화 색상 설정 ====================\n",
    "COLOR_MAP = {\n",
    "    \"Building\":   (0, 255, 255),  # 청록색 - 건물\n",
    "    \"Container\":  (255, 255,   0), # 노란색 - 컨테이너\n",
    "    \"Field\":      (255,   0, 255), # 자홍색 - 밭\n",
    "    \"Greenhouse\": (255,   0,   0), # 빨간색 - 온실\n",
    "    \"Tomb\":       (0,     0, 255),\n",
    "    \"Tree\":       (0,   255,   0),\n",
    "}\n",
    "# ────────────────────────────────────────────────────\n",
    "\n",
    "# ==================== 모델 초기화 ====================\n",
    "# 1) OWLv2 모델 & 프로세서 로드\n",
    "processor = Owlv2Processor.from_pretrained(\n",
    "    \"google/owlv2-large-patch14-ensemble\",\n",
    "    image_size=TARGET_SIZE  # 1008x1008 입력 크기\n",
    ")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\n",
    "    \"google/owlv2-large-patch14-ensemble\"\n",
    ").to(DEVICE).eval()  # 평가 모드로 설정\n",
    "\n",
    "# 2) 출력 폴더 만들기\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "font = ImageFont.load_default()  # 텍스트 표시용 폰트\n",
    "\n",
    "# ==================== 메인 처리 루프 ====================\n",
    "# 3) 이미지별 처리 - 각 이미지에 대해 이미지 기반 탐지 수행\n",
    "for img_path in IMAGE_DIR.glob(\"*\"):\n",
    "    # 지원하는 이미지 형식만 처리\n",
    "    if img_path.suffix.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n▶ Processing {img_path.name} …\")\n",
    "    \n",
    "    # ==================== 이미지 전처리 ====================\n",
    "    # 원본 이미지 로드 및 RGB 변환\n",
    "    orig = Image.open(img_path).convert(\"RGB\")\n",
    "    # OWLv2 모델 입력 크기로 리사이징\n",
    "    img = orig.resize((TARGET_SIZE, TARGET_SIZE), Image.BILINEAR)\n",
    "    w, h = img.size  # w == h == 1008\n",
    "\n",
    "    # ==================== 이미지 기반 탐지 결과 저장용 리스트 ====================\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "\n",
    "    # ==================== 4단계: 클래스별 이미지 기반 탐지 (배치 처리) ====================\n",
    "    # 각 클래스마다 대표 이미지들을 사용하여 유사한 객체 탐지\n",
    "    for cls in CLASS_NAMES:\n",
    "        # 클래스별 대표 이미지 경로들 가져오기\n",
    "        exemplar_paths = list((EACH_CLASSES_DIR/cls).glob(\"*\"))\n",
    "        if not exemplar_paths:\n",
    "            continue\n",
    "\n",
    "        # ==================== 대표 이미지들 로드 및 전처리 ====================\n",
    "        # 각 클래스의 대표 이미지들을 1008x1008로 리사이징\n",
    "        query_images = [\n",
    "            Image.open(p).convert(\"RGB\").resize((TARGET_SIZE, TARGET_SIZE), Image.BILINEAR)\n",
    "            for p in exemplar_paths\n",
    "        ]\n",
    "        # 텍스트 라벨과 이미지 배치 준비\n",
    "        texts = [cls] * len(query_images)  # 각 대표 이미지마다 클래스명\n",
    "        images_batch = [img] * len(query_images)  # 동일한 원본 이미지를 대표 이미지 개수만큼 복제\n",
    "\n",
    "        print(f\"  • Guided detection for class '{cls}' with {len(query_images)} exemplars\")\n",
    "\n",
    "        # ==================== OWLv2 이미지 기반 탐지 입력 준비 ====================\n",
    "        # 이미지 기반 탐지를 위한 특별한 입력 형식\n",
    "        inputs = processor(\n",
    "            images=images_batch,      # 원본 이미지들 (배치)\n",
    "            text=texts,              # 클래스명 텍스트들\n",
    "            query_images=query_images,  # 대표 이미지들 (쿼리)\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # GPU로 데이터 이동\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "        # ==================== 이미지 기반 탐지 추론 ====================\n",
    "        # GPU 메모리 사용량 최적화를 위한 no_grad 컨텍스트\n",
    "        with torch.no_grad():\n",
    "            # OWLv2의 이미지 기반 탐지 기능 사용\n",
    "            outputs = model.image_guided_detection(**inputs)\n",
    "            \n",
    "        # 이미지 기반 탐지 결과 후처리\n",
    "        results_batch = processor.post_process_image_guided_detection(\n",
    "            outputs=outputs,\n",
    "            target_sizes=torch.tensor([[h, w]] * len(images_batch), device=DEVICE),\n",
    "            threshold=GUIDED_THRESHOLD  # 이미지 기반 탐지 임계값\n",
    "        )\n",
    "\n",
    "        # ==================== 클래스별 탐지 결과 수집 ====================\n",
    "        # 각 대표 이미지별 탐지 결과를 전체 리스트에 추가\n",
    "        for res in results_batch:\n",
    "            for box, score in zip(res[\"boxes\"], res[\"scores\"]):\n",
    "                all_boxes.append(box)      # 바운딩 박스 좌표\n",
    "                all_scores.append(score)   # 신뢰도 점수\n",
    "                all_labels.append(cls)     # 클래스명\n",
    "\n",
    "    # ==================== 5단계: 텍스트 기반 탐지 폴백 ====================\n",
    "    # 이미지 기반 탐지에서 아무것도 찾지 못한 경우 텍스트 기반 탐지로 폴백\n",
    "    if not all_boxes:\n",
    "        print(f\"  [FALLBACK] no guided detections, running text-based zero-shot\")\n",
    "        \n",
    "        # ==================== 텍스트 기반 탐지 입력 준비 ====================\n",
    "        txt_inputs = processor(\n",
    "            text=[CLASS_NAMES],  # 모든 클래스명을 텍스트로\n",
    "            images=[img],        # 단일 이미지\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # GPU로 데이터 이동\n",
    "        txt_inputs = {k: v.to(DEVICE) for k, v in txt_inputs.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # ==================== 텍스트 기반 탐지 추론 ====================\n",
    "        with torch.no_grad():\n",
    "            txt_out = model(**txt_inputs)  # 일반적인 텍스트 기반 탐지\n",
    "            \n",
    "        # 텍스트 기반 탐지 결과 후처리\n",
    "        txt_res = processor.post_process_object_detection(\n",
    "            outputs=txt_out,\n",
    "            target_sizes=torch.tensor([[h, w]], device=DEVICE),\n",
    "            threshold=0.3  # 텍스트 기반 탐지 임계값\n",
    "        )[0]\n",
    "        \n",
    "        # 텍스트 기반 탐지 결과 처리\n",
    "        boxes2, scores2, labels2 = txt_res[\"boxes\"], txt_res[\"scores\"], txt_res[\"labels\"]\n",
    "        # NMS 적용\n",
    "        keep2 = nms(boxes2, scores2, IOU_THRESHOLD)\n",
    "        # 폴백 결과를 전체 리스트에 추가\n",
    "        for idx in keep2:\n",
    "            all_boxes.append(boxes2[idx])\n",
    "            all_scores.append(scores2[idx])\n",
    "            all_labels.append(CLASS_NAMES[int(labels2[idx])])\n",
    "\n",
    "    # ==================== 6단계: 최종 NMS 및 시각화 ====================\n",
    "    # 여전히 탐지 결과가 없는 경우 스킵\n",
    "    if not all_boxes:\n",
    "        print(f\"  [WARN] still no detections, skipping\")\n",
    "        continue\n",
    "\n",
    "    # 텐서로 변환\n",
    "    boxes_t  = torch.stack(all_boxes)   # 모든 바운딩 박스를 하나의 텐서로\n",
    "    scores_t = torch.stack(all_scores)  # 모든 점수를 하나의 텐서로\n",
    "    \n",
    "    # 최종 NMS 적용: 겹치는 박스들 중에서 가장 높은 점수의 박스만 유지\n",
    "    keep     = nms(boxes_t, scores_t, IOU_THRESHOLD)\n",
    "\n",
    "    # ==================== 시각화 및 JSON 작성 ====================\n",
    "    draw = ImageDraw.Draw(img)  # 이미지에 그리기 객체 생성\n",
    "    records = []  # JSON 저장용 결과 리스트\n",
    "\n",
    "    # NMS로 필터링된 최종 탐지 결과에 대해 시각화\n",
    "    for i in keep:\n",
    "        # ==================== 박스 좌표 처리 ====================\n",
    "        x1, y1, x2, y2 = boxes_t[i].tolist()\n",
    "        # 이미지 경계를 벗어나는 박스 좌표를 이미지 내부로 제한\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w, x2), min(h, y2)\n",
    "        \n",
    "        # 클래스 정보 및 색상 설정\n",
    "        label = all_labels[i]  # 클래스명\n",
    "        score = float(scores_t[i])  # 신뢰도 점수\n",
    "        color = COLOR_MAP.get(label, (255,255,255))  # 클래스별 색상\n",
    "\n",
    "        # ==================== 바운딩 박스 그리기 ====================\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "        \n",
    "        # ==================== 라벨 텍스트 표시 ====================\n",
    "        txt = f\"{label}: {score:.2f}\"  # 클래스명과 신뢰도 점수\n",
    "        # 텍스트 배경 박스 계산\n",
    "        tx0, ty0, tx1, ty1 = draw.textbbox((x1, y1), txt, font=font)\n",
    "        # 텍스트 배경 그리기\n",
    "        draw.rectangle([tx0, ty0, tx1, ty1], fill=color)\n",
    "        # 흰색 텍스트 그리기\n",
    "        draw.text((x1, y1), txt, fill=(255,255,255), font=font)\n",
    "\n",
    "        # ==================== JSON 기록용 데이터 추가 ====================\n",
    "        records.append({\n",
    "            \"label\": label,\n",
    "            \"score\": score,\n",
    "            \"box\": [round(x1,2), round(y1,2), round(x2,2), round(y2,2)]\n",
    "        })\n",
    "\n",
    "    # ==================== 7단계: 결과 파일 저장 ====================\n",
    "    out_img  = OUTPUT_DIR / img_path.name  # 시각화된 이미지 저장 경로\n",
    "    out_json = OUTPUT_DIR / f\"{img_path.stem}.json\"  # JSON 결과 저장 경로\n",
    "    \n",
    "    # 시각화된 이미지 저장\n",
    "    img.save(out_img)\n",
    "    # JSON 결과 저장 (UTF-8 인코딩, 한글 지원)\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(records, fp, ensure_ascii=False, indent=2)\n",
    "    print(f\"  [OK] Saved → {out_img.name}, {out_json.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3a597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owlv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
